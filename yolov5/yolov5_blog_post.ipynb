{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fcbdc-65d3-49d5-b8f9-63fe79c48bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5\n",
    "%cd yolov5\n",
    "%pip install -r requirements.txt\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d26215-efd0-4824-8aa5-88be1c39c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm datasets wandb joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a677be-4fd7-4b10-8641-627cecdc914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def download_dataset():\n",
    "    wider_face = load_dataset('wider_face', split='train')\n",
    "    print(\"Num images in wider_face training set: %i\" % (len(wider_face)))\n",
    "\n",
    "    img = np.array(wider_face[110]['image'], dtype=np.uint8)\n",
    "    faces = wider_face[110]['faces']\n",
    "    bboxes = faces['bbox']\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        rect = patches.Rectangle((bbox[0], bbox[1]),\n",
    "                                 bbox[2], bbox[3],\n",
    "                                 linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return wider_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b16293-03a0-43d3-8c8b-1fd390e42049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "from datasets import IterableDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _write_files(dataset, dst_dir: str, i: int):\n",
    "    data_point = dataset[i]\n",
    "    pil_img = data_point['image']\n",
    "    label = data_point['faces']\n",
    "    img_filename = str(i) + \".png\"\n",
    "\n",
    "    dst_image_file = os.path.join(dst_dir, \"images/%s\" % (img_filename))\n",
    "    dst_label_file = os.path.join(dst_dir, \"labels/%s\" % (img_filename.replace(\".png\", \".txt\")))\n",
    "    if os.path.exists(dst_label_file):\n",
    "        return\n",
    "\n",
    "    class_name = \"face\"  # we're only detecting faces, so these are constants\n",
    "    class_id = 0 \n",
    "    img_width, img_height = pil_img.size\n",
    "    with open(dst_label_file, \"w\") as wobj:\n",
    "        for bbox in label['bbox']:\n",
    "            cx = (bbox[0] + (bbox[2]/2.0)) / img_width\n",
    "            cy = (bbox[1] + (bbox[3]/2.0)) / img_height\n",
    "\n",
    "            # output annotation is: class_id, center_x, center_y, box_width, box_height,\n",
    "            # image width and height normalized to (0, 1)\n",
    "            output_line = \"%d %f %f %f %f\\n\" % (class_id, cx, cy, bbox[2]/img_width, bbox[3]/img_height)\n",
    "            wobj.write(output_line)\n",
    "    pil_img.save(dst_image_file)\n",
    "\n",
    "\n",
    "def convert_to_yolov5_format(\n",
    "    dataset: IterableDataset,\n",
    "    dst_dir: str,\n",
    ") -> None:\n",
    "    Path(os.path.join(dst_dir, \"images\")).mkdir(parents=True, exist_ok=True)\n",
    "    Path(os.path.join(dst_dir, \"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    Parallel(n_jobs=32)(delayed(_write_files)(dataset, dst_dir, i)\n",
    "                        for i in tqdm(range(len(dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b415e71-4e42-4482-bc1d-e898780fc111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yolov5_dataset_yaml(yolo_train_dir: str, yolo_test_dir: str):\n",
    "    yaml_file = \"./yolov5/data/wider_face.yaml\"\n",
    "    train_images_dir = os.path.join(yolo_train_dir, \"images\")\n",
    "    val_images_dir = os.path.join(yolo_test_dir, \"images\")\n",
    "\n",
    "    classes = ['Face']\n",
    "    names_str = \"\"\n",
    "    for item in classes:\n",
    "        names_str = names_str + \", \\'%s\\'\" % item\n",
    "    names_str = \"names: [\" + names_str[1:] + \"]\"\n",
    "\n",
    "    with open(yaml_file, \"w\") as wobj:\n",
    "        wobj.write(\"train: %s\\n\" % train_images_dir)\n",
    "        wobj.write(\"val: %s\\n\" % val_images_dir)\n",
    "        wobj.write(\"nc: %d\\n\" % len(classes))\n",
    "        wobj.write(names_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe4f31c-3380-4ab2-928b-bf04a4ad4914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "  \n",
    "from download_dataset import download_dataset\n",
    "from convert_to_yolov5_format import convert_to_yolov5_format\n",
    "from create_yolov5_dataset_yaml import create_yolov5_dataset_yaml\n",
    "from yolov5.utils.downloads import attempt_download\n",
    "\n",
    "\n",
    "wider_face = download_dataset()\n",
    "\n",
    "convert_to_yolov5_format(wider_face, dst_dir=\"./yolov5/data/train\")\n",
    "convert_to_yolov5_format(wider_face, dst_dir=\"./yolov5/data/test\")\n",
    "create_yolov5_dataset_yaml(\"./yolov5/data/train\", \"./yolov5/data/test\")\n",
    "\n",
    "sys.path.append('yolov5')\n",
    "attempt_download('yolov5/weights/yolov5s.pt')\n",
    "attempt_download('yolov5/weights/yolov5m.pt')\n",
    "attempt_download('yolov5/weights/yolov5l.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00468f-08fa-4003-826d-75bfb2df2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node 2 train.py --data data/wider_face.yaml --batch-size 32 --epochs 10 --img-size 768 --project runs/train --name wider_face --weights weights/yolov5s.pt --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfd4806-07af-4acf-86e1-fd44097a2143",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node 2 train.py --data data/wider_face.yaml --batch-size 32 --epochs 10 --img-size 768 --project runs/train --name wider_face --weights weights/yolov5m.pt --device 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01d37e-e61c-4fe3-980b-6414e2018f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node 2 train.py --data data/wider_face.yaml --batch-size 32 --epochs 10 --img-size 768 --project runs/train --name wider_face --weights weights/yolov5l.pt --device 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
